import streamlit as st
from domain.entities.ab_tester import ABTester
from domain.entities.variation import Variation
from domain.use_cases.ab_statistical_validator import ABStatisticalValidator 

class ResultsComponent:
    def __init__(self, tester: ABTester, variation: Variation): # Changed signature
        self.tester = tester
        self.variation = variation # Store variation entity

        if not self.tester or not self.variation:
            st.error("As entidades Tester e Variation s√£o obrigat√≥rias para ResultsComponent.")
            self.results = {} # Initialize as empty dict to avoid errors
            return

        # Perform statistical validation internally
        validator = ABStatisticalValidator(variation=self.variation, tester=self.tester)
        self.results = validator.get_statistical_results()

        # Ensure conversion rates are in self.results for _display_confidence_intervals
        # if they are not already added by ABStatisticalValidator
        if 'control_conversion_rate' not in self.results:
            self.results['control_conversion_rate'] = self.variation.conversion_rate_a
        if 'variation_conversion_rate' not in self.results:
            self.results['variation_conversion_rate'] = self.variation.conversion_rate_b

    def _display_main_result(self):
        st.subheader("Resultado Principal")
        p_value = self.results.get("p_value", 1.0)
        
        # Correctly calculate significance_level from desired_confidence_level (e.g., 95.0 -> 0.05)
        significance_level = 1.0 - (self.tester.desired_confidence_level / 100.0)

        if p_value < significance_level:
            st.success(f"### üéâ Resultado Significante!")
            # Use self.tester.desired_confidence_level for the message consistency
            st.write(f"Com uma confian√ßa de **{self.tester.desired_confidence_level}%**, a Varia√ß√£o (B) mostrou um desempenho estatisticamente diferente do Controle (A). (p-valor: {p_value:.4f})")
        else:
            st.warning(f"### ü§î Resultado N√£o Significante")
            st.write(f"N√£o h√° evid√™ncia estat√≠stica suficiente para afirmar uma diferen√ßa de desempenho entre a Varia√ß√£o (B) e o Controle (A) com o n√≠vel de confian√ßa desejado de **{self.tester.desired_confidence_level}%**. (p-valor: {p_value:.4f})")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Uplift da Convers√£o", f"{self.results.get('conversion_rate_uplift', 0):.2%}")
        col2.metric("P-Valor", f"{p_value:.4f}")
        # current_confidence is calculated by ABStatisticalValidator based on p_value
        col3.metric("Confian√ßa do Resultado", f"{self.results.get('current_confidence', 0):.2%}")
    
    def _display_confidence_intervals(self):
        with st.expander("üîç An√°lise de Uplift e Intervalos de Confian√ßa"):
            st.markdown("""
            O **Intervalo de Confian√ßa** mostra a faixa prov√°vel da verdadeira taxa de convers√£o para cada grupo. Se os intervalos **n√£o se sobrep√µem**, √© um forte indicativo de que a diferen√ßa entre eles √© real.
            """)
            
            # These should now be correctly populated in self.results by __init__
            p_a = self.results.get('control_conversion_rate', 0)
            p_b = self.results.get('variation_conversion_rate', 0)
            lower_a = self.results.get('control_lower_bound', 0)
            upper_a = self.results.get('control_upper_bound', 0)
            lower_b = self.results.get('variation_lower_bound', 0)
            upper_b = self.results.get('variation_upper_bound', 0)
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"**Controle (A): {p_a:.2%}**")
                st.write(f"Intervalo: `{lower_a:.2%}` a `{upper_a:.2%}`")
            with col2:
                st.markdown(f"**Varia√ß√£o (B): {p_b:.2%}**")
                st.write(f"Intervalo: `{lower_b:.2%}` a `{upper_b:.2%}`")

    def _display_test_validity(self):
        with st.expander("‚úÖ An√°lise de Validade e Poder do Teste"):
            st.markdown("""
            Esta se√ß√£o verifica se os resultados do seu teste s√£o confi√°veis. Verificamos o **Poder Estat√≠stico** (a capacidade de detectar um efeito real) e o **SRM** (se o tr√°fego foi dividido corretamente).
            """)

            # Poder do Teste Observado
            power = self.results.get("observed_test_power", 0)
            st.subheader("Poder do Teste Observado")
            if isinstance(power, (float, int)):
                st.progress(int(power * 100))
                st.write(f"O poder observado do seu teste foi de **{power:.2%}**.")
                if power >= 0.8:
                    st.success("**Excelente!** Seu teste teve uma alta probabilidade de detectar uma diferen√ßa real, caso ela exista.")
                else:
                    st.warning("**Aten√ß√£o!** O poder do teste foi baixo. Isso significa que, mesmo que houvesse uma diferen√ßa real, o teste tinha uma baixa probabilidade de detect√°-la. Para testes futuros, considere aumentar o n√∫mero de visitantes.")
            else:
                st.write("Poder do teste n√£o p√¥de ser calculado.")
            
            st.divider()

            # Valida√ß√£o de SRM
            srm_results = self.results.get("srm_results", {})
            has_srm = srm_results.get("has_srm", False)
            srm_p_value = srm_results.get("srm_p_value", 1.0)
            st.subheader("SRM (Sample Ratio Mismatch)")
            if not has_srm:
                st.success(f"**N√ÉO H√Å SRM.** (P-valor do SRM: {srm_p_value:.3f})")
                st.write("A divis√£o de tr√°fego entre os grupos parece correta. Os resultados do teste s√£o confi√°veis nesse quesito.")
            else:
                st.error(f"**ALERTA DE SRM!** (P-valor do SRM: {srm_p_value:.3f})")
                st.write("A propor√ß√£o de visitantes entre os grupos Controle e Varia√ß√£o √© inesperadamente diferente. Isso pode indicar um problema na configura√ß√£o do teste e invalida os resultados. Verifique sua ferramenta de A/B testing.")

    def render(self):
        if not self.results: # Check if results were properly initialized
            st.error("N√£o foi poss√≠vel gerar o relat√≥rio de resultados. Verifique os dados de entrada.")
            return
            
        st.title("üìä Relat√≥rio de Resultados")
        self._display_main_result()
        st.divider()
        self._display_confidence_intervals()
        self._display_test_validity()